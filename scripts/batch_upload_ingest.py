#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –º–∞—Å—Å–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∏–Ω–≥–µ—Å—Ç–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–ü—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥:
  - –ø—É—Ç—å –∫ –æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É (.docx, .pdf, .xlsx)
  - –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ñ–∞–π–ª–∞–º–∏

–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: API –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ .docx, .pdf, .xlsx. 
–§–∞–π–ª—ã —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º .doc (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç Word) –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º.

–î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞:
  1. –°–æ–∑–¥–∞—ë—Ç study/document/version (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
  2. –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª —á–µ—Ä–µ–∑ upload API
  3. –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –∏–Ω–≥–µ—Å—Ç–∏–∏
  4. –û–∂–∏–¥–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∏–Ω–≥–µ—Å—Ç–∏–∏

–í –∫–æ–Ω—Ü–µ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º.

–û–ø—Ü–∏—è --resume:
  –ü–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (–ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ SHA256 –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö)
  –∏ –Ω–∞—á–∏–Ω–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É —Å –ø–µ—Ä–≤–æ–≥–æ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞. –ü–æ–ª–µ–∑–Ω–æ –ø—Ä–∏ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–∏
  –º–∞—Å—Å–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ - –º–æ–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –º–µ—Å—Ç–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.
"""

import argparse
import csv
import hashlib
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

try:
    import requests
except ImportError:
    print("–û–®–ò–ë–ö–ê: –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ requests. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install requests")
    sys.exit(1)

try:
    from dotenv import load_dotenv
except ImportError:
    load_dotenv = None

ALLOWED_EXTENSIONS = {".docx", ".pdf", ".xlsx"}


class HTTPError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è HTTP –æ—à–∏–±–æ–∫."""
    pass


def die(msg: str, code: int = 1) -> None:
    """–ó–∞–≤–µ—Ä—à–∞–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å –æ—à–∏–±–∫–æ–π."""
    print(f"–û–®–ò–ë–ö–ê: {msg}", file=sys.stderr)
    sys.exit(code)


def ensure_dir(path: Path) -> None:
    """–°–æ–∑–¥–∞—ë—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç."""
    path.mkdir(parents=True, exist_ok=True)


def http_json(method: str, url: str, *, json_body: Any = None, params: Dict[str, Any] | None = None, timeout: int = 30, raise_on_error: bool = True) -> Any:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç HTTP –∑–∞–ø—Ä–æ—Å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON –æ—Ç–≤–µ—Ç."""
    headers = {"Accept": "application/json"}
    if json_body is not None:
        headers["Content-Type"] = "application/json"
    r = requests.request(method, url, headers=headers, json=json_body, params=params, timeout=timeout)
    if r.status_code >= 400:
        try:
            body = r.json()
            error_msg = json.dumps(body, indent=2, ensure_ascii=False)
        except Exception:
            error_msg = r.text
        error_text = f"{method} {url} -> {r.status_code}\n{error_msg}"
        if raise_on_error:
            die(error_text)
        else:
            raise HTTPError(error_text)
    if r.text.strip() == "":
        return None
    return r.json()


def upload_file(api_base: str, version_id: str, file_path: Path) -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª –¥–ª—è –≤–µ—Ä—Å–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
    url = f"{api_base}/api/document-versions/{version_id}/upload"
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º content-type –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
    ext = file_path.suffix.lower()
    content_types = {
        ".doc": "application/msword",
        ".docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        ".pdf": "application/pdf",
        ".xlsx": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    }
    content_type = content_types.get(ext, "application/octet-stream")
    
    with open(file_path, "rb") as f:
        files = {"file": (file_path.name, f, content_type)}
        r = requests.post(url, files=files, headers={"Accept": "application/json"}, timeout=300)
    
    if r.status_code >= 400:
        die(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {r.status_code}: {r.text}")
    
    return r.json()


def get_version_status(api_base: str, version_id: str) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å –≤–µ—Ä—Å–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
    url = f"{api_base}/api/document-versions/{version_id}"
    return http_json("GET", url, timeout=30)


def start_ingestion(api_base: str, version_id: str, force: bool = True) -> Tuple[bool, Optional[str]]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –∏–Ω–≥–µ—Å—Ç–∏–∏ –¥–ª—è –≤–µ—Ä—Å–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        (was_started, current_status) - –±—ã–ª–∞ –ª–∏ –∏–Ω–≥–µ—Å—Ç–∏—è –∑–∞–ø—É—â–µ–Ω–∞ –∏ —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å
        –ï—Å–ª–∏ –∏–Ω–≥–µ—Å—Ç–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (False, "processing")
    """
    url = f"{api_base}/api/document-versions/{version_id}/ingest"
    if force:
        url += "?force=true"
    
    try:
        # –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–µ—Ä–≤–µ—Ä—ã —Ç—Ä–µ–±—É—é—Ç –ø—É—Å—Ç–æ–µ —Ç–µ–ª–æ –∏–ª–∏ –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç –µ–≥–æ
        r = requests.post(url, json={}, headers={"Accept": "application/json"}, timeout=120)
    except Exception:
        # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –±–µ–∑ json
        r = requests.post(url, headers={"Accept": "application/json"}, timeout=120)
    
    if r.status_code == 409:
        # –ö–æ–Ω—Ñ–ª–∏–∫—Ç: –∏–Ω–≥–µ—Å—Ç–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è
        try:
            error_body = r.json()
            current_status = error_body.get("details", {}).get("current_status", "processing")
            return False, current_status
        except Exception:
            return False, "processing"
    
    if r.status_code >= 400:
        die(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∏–Ω–≥–µ—Å—Ç–∏–∏ {r.status_code}: {r.text}")
    
    return True, None


def poll_ingestion_status(
    api_base: str, version_id: str, timeout_sec: int = 600
) -> Tuple[str, Dict[str, Any]]:
    """
    –û–∂–∏–¥–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∏–Ω–≥–µ—Å—Ç–∏–∏, –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä—è—è —Å—Ç–∞—Ç—É—Å.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        (final_status, version_data)
    """
    deadline = time.time() + timeout_sec
    url = f"{api_base}/api/document-versions/{version_id}"
    
    last_status = None
    while True:
        try:
            v = http_json("GET", url, timeout=30)
            status = v.get("ingestion_status")
            
            # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç—É—Å, –µ—Å–ª–∏ –æ–Ω –∏–∑–º–µ–Ω–∏–ª—Å—è
            if status != last_status:
                print(f"    –°—Ç–∞—Ç—É—Å –∏–Ω–≥–µ—Å—Ç–∏–∏: {status}")
                last_status = status
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—É—Å—ã
            if status in ("ready", "needs_review", "failed"):
                return status, v
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞–π–º–∞—É—Ç
            if time.time() > deadline:
                die(f"–¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∏–Ω–≥–µ—Å—Ç–∏–∏ (>{timeout_sec} —Å–µ–∫)")
            
            time.sleep(2)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∫–∞–∂–¥—ã–µ 2 —Å–µ–∫—É–Ω–¥—ã
            
        except KeyboardInterrupt:
            print("\n–ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            sys.exit(1)
        except Exception as e:
            print(f"    –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å—Ç–∞—Ç—É—Å–∞: {e}")
            time.sleep(2)


def create_study(api_base: str, workspace_id: str, study_code: str, title: str, *, raise_on_error: bool = True) -> str:
    """–°–æ–∑–¥–∞—ë—Ç –Ω–æ–≤–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç study_id."""
    url = f"{api_base}/api/studies"
    body = {
        "workspace_id": workspace_id,
        "study_code": study_code,
        "title": title,
        "status": "active",
    }
    study = http_json("POST", url, json_body=body, timeout=30, raise_on_error=raise_on_error)
    return study["id"]


def create_document(api_base: str, study_id: str, doc_type: str, title: str) -> str:
    """–°–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç document_id."""
    url = f"{api_base}/api/studies/{study_id}/documents"
    body = {
        "doc_type": doc_type,
        "title": title,
        "lifecycle_status": "draft",
    }
    doc = http_json("POST", url, json_body=body, timeout=30)
    return doc["id"]


def find_study_id_by_code(api_base: str, workspace_id: str, study_code: str) -> Optional[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç id –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ –∫–æ–¥—É, –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ."""
    try:
        studies = http_json(
            "GET",
            f"{api_base}/api/studies",
            params={"workspace_id": workspace_id},
            timeout=60,
            raise_on_error=False,
        )
        if isinstance(studies, list):
            for study in studies:
                if study.get("study_code") == study_code:
                    return study.get("id")
    except Exception as e:
        print(f"    [WARN] –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ {study_code}: {e}")
    return None


def ensure_study(api_base: str, workspace_id: str, study_code: str, title: str) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç id –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, —Å–æ–∑–¥–∞—ë—Ç –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏."""
    existing = find_study_id_by_code(api_base, workspace_id, study_code)
    if existing:
        return existing
    try:
        return create_study(api_base, workspace_id, study_code, title, raise_on_error=True)
    except HTTPError as e:
        # –ù–∞ —Å–ª—É—á–∞–π –≥–æ–Ω–∫–∏: –ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ—á–∏—Ç–∞—Ç—å
        print(f"    [WARN] –°–æ–∑–¥–∞–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è {study_code} –≤–µ—Ä–Ω—É–ª–æ –æ—à–∏–±–∫—É: {e}. –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ.")
        retry = find_study_id_by_code(api_base, workspace_id, study_code)
        if retry:
            return retry
        raise


def find_protocol_document(api_base: str, study_id: str) -> Optional[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç id –ø—Ä–æ—Ç–æ–∫–æ–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω."""
    try:
        documents = http_json("GET", f"{api_base}/api/studies/{study_id}/documents", timeout=60, raise_on_error=False)
        if isinstance(documents, list):
            for doc in documents:
                if doc.get("doc_type") == "protocol":
                    return doc.get("id")
    except Exception as e:
        print(f"    [WARN] –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è {study_id}: {e}")
    return None


def ensure_protocol_document(api_base: str, study_id: str, title: str) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç id –¥–æ–∫—É–º–µ–Ω—Ç–∞-–ø—Ä–æ—Ç–æ–∫–æ–ª–∞, —Å–æ–∑–¥–∞—ë—Ç –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏."""
    existing = find_protocol_document(api_base, study_id)
    if existing:
        return existing
    return create_document(api_base, study_id, "protocol", title)


def download_json_to_file(url: str, target_path: Path) -> None:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç JSON –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —Ñ–∞–π–ª."""
    try:
        r = requests.get(url, headers={"Accept": "application/json"}, timeout=120)
        if r.status_code >= 400:
            print(f"    [WARN] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å {url}: {r.status_code} {r.text}")
            return
        data = r.json()
        ensure_dir(target_path.parent)
        with open(target_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"    [WARN] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")


def save_benchmark_artifacts(
    api_base: str,
    project_root: Path,
    study_code: str,
    version_number: int,
    study_id: str,
    version_id: str,
) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ñ–∞–∫—Ç–æ–≤, SoA –∏ topic_evidence –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –∏–Ω–≥–µ—Å—Ç–∏–∏."""
    target_dir = project_root / "benchmark_results" / study_code
    ensure_dir(target_dir)
    facts_path = target_dir / f"v{version_number}_facts.json"
    soa_path = target_dir / f"v{version_number}_soa.json"
    topics_path = target_dir / f"v{version_number}_topics.json"
    
    # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–∫—Ç—ã –∏ –æ–±–æ–≥–∞—â–∞–µ–º –∏—Ö found_in_preferred_topic
    facts_data = http_json("GET", f"{api_base}/api/studies/{study_id}/facts", timeout=120, raise_on_error=False)
    if facts_data:
        # –î–æ–±–∞–≤–ª—è–µ–º found_in_preferred_topic –∏–∑ meta_json
        for fact in facts_data:
            meta_json = fact.get("meta_json") or {}
            fact["found_in_preferred_topic"] = meta_json.get("found_in_preferred_topic", False)
        ensure_dir(facts_path.parent)
        with open(facts_path, "w", encoding="utf-8") as f:
            json.dump(facts_data, f, ensure_ascii=False, indent=2)
    
    download_json_to_file(f"{api_base}/api/document-versions/{version_id}/soa", soa_path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º topic_evidence
    download_json_to_file(f"{api_base}/api/document-versions/{version_id}/topics", topics_path)


def create_document_version(api_base: str, document_id: str, version_label: str, effective_date: Optional[str] = None) -> str:
    """–°–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç version_id."""
    url = f"{api_base}/api/documents/{document_id}/versions"
    body = {"version_label": version_label}
    if effective_date is not None:
        body["effective_date"] = effective_date
    ver = http_json("POST", url, json_body=body, timeout=30)
    return ver["id"]


def calculate_file_sha256(file_path: Path) -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç SHA256 —Ö–µ—à —Ñ–∞–π–ª–∞."""
    sha256_hash = hashlib.sha256()
    with open(file_path, "rb") as f:
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –±–ª–æ–∫–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()


def get_processed_sha256_set(api_base: str, workspace_id: str, debug: bool = False) -> Set[str]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ SHA256 —Ö–µ—à–µ–π –≤—Å–µ—Ö –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç set —Å SHA256 —Ö–µ—à–∞–º–∏ (–≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è).
    """
    processed_hashes: Set[str] = set()
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ workspace
        url = f"{api_base}/api/studies"
        params = {"workspace_id": workspace_id}
        
        try:
            studies = http_json("GET", url, params=params, timeout=60, raise_on_error=False)
        except Exception as e:
            # –ü–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ–º HTTPError –∏ –¥—Ä—É–≥–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
            print(f"    –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏–∑ API: {e}")
            print(f"    URL: {url}")
            print(f"    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {params}")
            print(f"    –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
            return processed_hashes
        
        if not isinstance(studies, list):
            print(f"    –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π")
            return processed_hashes
        
        print(f"    –ù–∞–π–¥–µ–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π: {len(studies)}")
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        for study in studies:
            study_id = study.get("id")
            if not study_id:
                continue
            
            try:
                # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
                docs_url = f"{api_base}/api/studies/{study_id}/documents"
                documents = http_json("GET", docs_url, timeout=60)
                
                if not isinstance(documents, list):
                    continue
                
                # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ–ª—É—á–∞–µ–º –≤–µ—Ä—Å–∏–∏
                for doc in documents:
                    doc_id = doc.get("id")
                    if not doc_id:
                        continue
                    
                    try:
                        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä—Å–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                        versions_url = f"{api_base}/api/documents/{doc_id}/versions"
                        versions = http_json("GET", versions_url, timeout=60)
                        
                        if not isinstance(versions, list):
                            continue
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º SHA256 –∏–∑ –≤–µ—Ä—Å–∏–π
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã —Å –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–π –∏–Ω–≥–µ—Å—Ç–∏–µ–π (ready –∏–ª–∏ needs_review)
                        for version in versions:
                            sha256 = version.get("source_sha256")
                            ingestion_status = version.get("ingestion_status")
                            
                            if debug:
                                version_id = version.get("id", "unknown")
                                print(f"      –í–µ—Ä—Å–∏—è {version_id}: SHA256={sha256[:16] if sha256 else 'None'}..., —Å—Ç–∞—Ç—É—Å={ingestion_status}")
                            
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã —Å —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–π –∏–Ω–≥–µ—Å—Ç–∏–µ–π
                            # (ready –∏–ª–∏ needs_review). –§–∞–π–ª—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º failed –∏–ª–∏ processing
                            # –±—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∑–∞–Ω–æ–≤–æ
                            if sha256 and ingestion_status in ("ready", "needs_review"):
                                processed_hashes.add(sha256.lower())
                                if debug:
                                    print(f"        -> –î–æ–±–∞–≤–ª–µ–Ω –≤ —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö")
                            elif debug and sha256:
                                print(f"        -> –ü—Ä–æ–ø—É—â–µ–Ω (—Å—Ç–∞—Ç—É—Å {ingestion_status}, –Ω–µ ready/needs_review)")
                    
                    except Exception as e:
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≤–µ—Ä—Å–∏–π –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                        continue
            
            except Exception as e:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
                continue
        
        print(f"    –ù–∞–π–¥–µ–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (ready/needs_review): {len(processed_hashes)}")
        if debug and processed_hashes:
            print(f"    –ü—Ä–∏–º–µ—Ä—ã —Ö–µ—à–µ–π –∏–∑ –±–∞–∑—ã (–ø–µ—Ä–≤—ã–µ 5):")
            for i, h in enumerate(list(processed_hashes)[:5], 1):
                print(f"      {i}. {h[:32]}...")
        return processed_hashes
    
    except Exception as e:
        print(f"    –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {e}")
        print(f"    –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
        return processed_hashes


def _warn_doc_files(files: List[Path]) -> None:
    if not files:
        return
    print(f"\n–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª(–æ–≤) —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º .doc (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç Word):")
    for doc_file in files[:10]:
        print(f"  - {doc_file}")
    if len(files) > 10:
        print(f"  ... –∏ –µ—â—ë {len(files) - 10} —Ñ–∞–π–ª(–æ–≤)")
    print("  API –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ .docx, .pdf, .xlsx. –≠—Ç–∏ —Ñ–∞–π–ª—ã –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã.")
    print("  –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª—ã –≤ .docx –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π.\n")


def discover_study_files(path: Path) -> Dict[str, List[Path]]:
    """–°—Ç—Ä–æ–∏—Ç —Å–ª–æ–≤–∞—Ä—å {study_code: [—Ñ–∞–π–ª—ã]} c —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–æ–π –ø–æ –¥–∞—Ç–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è."""
    studies: Dict[str, List[Path]] = {}
    
    if path.is_file():
        ext = path.suffix.lower()
        if ext == ".doc":
            _warn_doc_files([path])
            return {}
        if ext not in ALLOWED_EXTENSIONS:
            die(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {ext}. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: {', '.join(ALLOWED_EXTENSIONS)}")
        study_code = path.parent.name or path.stem
        studies[study_code] = [path]
        return studies
    
    if not path.is_dir():
        die(f"–ü—É—Ç—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {path}")
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Å—Ü–µ–Ω–∞—Ä–∏–π: –ø–æ–¥–∫–∞—Ç–∞–ª–æ–≥–∏ ‚Äî —ç—Ç–æ study_code
    for sub in sorted(path.iterdir()):
        if not sub.is_dir():
            continue
        study_code = sub.name
        study_files = [f for f in sub.rglob("*") if f.is_file() and f.suffix.lower() in ALLOWED_EXTENSIONS]
        doc_files = [f for f in sub.rglob("*.doc") if f.is_file()]
        
        if doc_files:
            _warn_doc_files(doc_files)
        
        if study_files:
            study_files.sort(key=os.path.getmtime)
            studies[study_code] = study_files
    
    # –§–æ–ª–±—ç–∫: –µ—Å–ª–∏ –≤–Ω—É—Ç—Ä–∏ –±–∞–∑–æ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –Ω–µ—Ç –ø–æ–¥–∫–∞—Ç–∞–ª–æ–≥–æ–≤, –Ω–æ –µ—Å—Ç—å —Ñ–∞–π–ª—ã
    if not studies:
        direct_files = [f for f in path.rglob("*") if f.is_file() and f.suffix.lower() in ALLOWED_EXTENSIONS]
        doc_files = [f for f in path.rglob("*.doc") if f.is_file()]
        if doc_files:
            _warn_doc_files(doc_files)
        if direct_files:
            direct_files.sort(key=os.path.getmtime)
            studies[path.name] = direct_files
    
    return studies


def filter_processed_by_hash(study_files: Dict[str, List[Path]], processed_hashes: Set[str]) -> Tuple[Dict[str, List[Path]], int]:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (ready/needs_review) –ø–æ SHA256."""
    if not processed_hashes:
        return study_files, 0
    
    filtered: Dict[str, List[Path]] = {}
    skipped = 0
    
    for study_code, files in study_files.items():
        for file_path in files:
            try:
                file_hash = calculate_file_sha256(file_path)
            except Exception as e:
                print(f"  [WARN] –û—à–∏–±–∫–∞ —Ä–∞—Å—á—ë—Ç–∞ SHA256 –¥–ª—è {file_path}: {e}. –§–∞–π–ª –±—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω.")
                filtered.setdefault(study_code, []).append(file_path)
                continue
            
            if file_hash.lower() in processed_hashes:
                skipped += 1
                continue
            filtered.setdefault(study_code, []).append(file_path)
    
    filtered = {k: v for k, v in filtered.items() if v}
    return filtered, skipped


def extract_detailed_stats(version_data: Dict[str, Any]) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —à–∞–≥–∞–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏–∏."""
    summary = version_data.get("ingestion_summary_json", {})
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ SoA (Schedule of Activities)
    soa_stats = {
        "detected": False,
        "confidence": None,
        "visits_count": 0,
        "procedures_count": 0,
        "matrix_cells": 0,
    }
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ SoA —Ñ–∞–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ ingestion_summary_json
    soa_facts = summary.get("soa_facts_written", {})
    if isinstance(soa_facts, dict):
        soa_stats["detected"] = (
            soa_facts.get("visits", False) or
            soa_facts.get("procedures", False) or
            soa_facts.get("matrix", False)
        )
        counts = soa_facts.get("counts", {})
        soa_stats["visits_count"] = counts.get("visits", 0)
        soa_stats["procedures_count"] = counts.get("procedures", 0)
        soa_stats["matrix_cells"] = counts.get("matrix", 0)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞–∫–∂–µ —á–µ—Ä–µ–∑ –º–µ—Ç—Ä–∏–∫–∏
    metrics = summary.get("metrics", {})
    soa_metrics = metrics.get("soa", {})
    if soa_metrics.get("found"):
        soa_stats["detected"] = True
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä–µ–Ω—å summary_json (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç), –ø–æ—Ç–æ–º –º–µ—Ç—Ä–∏–∫–∏
        soa_stats["confidence"] = summary.get("soa_confidence") or soa_metrics.get("table_score")
        if soa_stats["visits_count"] == 0:
            soa_stats["visits_count"] = soa_metrics.get("visits_count", 0)
        if soa_stats["procedures_count"] == 0:
            soa_stats["procedures_count"] = soa_metrics.get("procedures_count", 0)
    else:
        # –î–∞–∂–µ –µ—Å–ª–∏ SoA –Ω–µ –Ω–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑ –º–µ—Ç—Ä–∏–∫–∏, –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä–µ–Ω—å summary_json
        if summary.get("soa_confidence") is not None:
            soa_stats["confidence"] = summary.get("soa_confidence")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ Chunks (Narrative Index)
    chunks_stats = {
        "created": summary.get("chunks_created", 0),
        "anchors_per_chunk_avg": None,
    }
    chunk_metrics = metrics.get("chunks", {})
    if chunk_metrics:
        chunks_stats["created"] = chunk_metrics.get("total", chunks_stats["created"])
        anchors_count = summary.get("anchors_created", 0)
        if chunks_stats["created"] > 0 and anchors_count > 0:
            chunks_stats["anchors_per_chunk_avg"] = round(anchors_count / chunks_stats["created"], 2)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ Facts (Rules-first)
    facts_stats = {
        "total_extracted": summary.get("facts_extracted_total") or summary.get("facts_count", 0),
        "validated_count": summary.get("facts_validated_count", 0),
        "conflicting_count": summary.get("facts_conflicting_count", 0),
        "needs_review": [],
        "by_type": {},
    }
    facts_metrics = metrics.get("facts", {})
    if facts_metrics:
        facts_stats["total_extracted"] = facts_metrics.get("total", facts_stats["total_extracted"])
        facts_stats["validated_count"] = facts_metrics.get("validated_count", facts_stats["validated_count"])
        facts_stats["conflicting_count"] = facts_metrics.get("conflicting_count", facts_stats["conflicting_count"])
        facts_stats["needs_review"] = facts_metrics.get("needs_review_list", [])
        facts_stats["by_type"] = facts_metrics.get("by_type", {})
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ Topic Mapping
    topics_stats = {
        "mapped_count": summary.get("topics_mapped_count", 0),
        "mapped_rate": summary.get("topics_mapped_rate", 0.0),
        "total_topics": summary.get("topics", {}).get("total_topics", 15) if isinstance(summary.get("topics"), dict) else 15,
    }
    # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ topics –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö
    topics_metrics = metrics.get("topics", {})
    if topics_metrics:
        topics_stats["mapped_count"] = topics_metrics.get("mapped_count", topics_stats["mapped_count"])
        topics_stats["mapped_rate"] = topics_metrics.get("mapped_rate", topics_stats["mapped_rate"])
        topics_stats["total_topics"] = topics_metrics.get("total_topics", topics_stats["total_topics"])
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ Section Mapping
    mapping_stats = {
        "sections_mapped": summary.get("sections_mapped_count", 0),
        "needs_review": summary.get("sections_needs_review_count", 0),
        "warnings": summary.get("mapping_warnings", []),
    }
    
    # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ docx_summary, –µ—Å–ª–∏ –µ—Å—Ç—å
    docx_summary = summary.get("docx_summary") or {}
    if docx_summary:
        if mapping_stats["sections_mapped"] == 0:
            mapping_stats["sections_mapped"] = docx_summary.get("sections_mapped_count", 0)
        if mapping_stats["needs_review"] == 0:
            mapping_stats["needs_review"] = docx_summary.get("sections_needs_review_count", 0)
        if not mapping_stats["warnings"]:
            mapping_stats["warnings"] = docx_summary.get("mapping_warnings", [])
    
    return {
        "soa": soa_stats,
        "chunks": chunks_stats,
        "facts": facts_stats,
        "topics": topics_stats,
        "section_mapping": mapping_stats,
    }


def process_file(
    api_base: str,
    workspace_id: str,
    file_path: Path,
    study_code: Optional[str] = None,
    study_id: Optional[str] = None,
    document_id: Optional[str] = None,
    version_id: Optional[str] = None,
    create_new_study: bool = True,
    version_label: str = "v1.0",
    ingestion_timeout: int = 600,
) -> Tuple[str, bool, Dict[str, Any]]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª: —Å–æ–∑–¥–∞—ë—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ), –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –∏–Ω–≥–µ—Å—Ç–∏—é.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        (version_id, success, stats)
    """
    file_name = file_path.name
    
    try:
        # –°–æ–∑–¥–∞—ë–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞
        if not version_id:
            if not study_id or create_new_study:
                # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
                actual_study_code = study_code or f"BATCH-{int(time.time())}-{file_path.stem[:30]}"
                study_title = f"Batch Upload: {file_path.stem}"
                study_id = create_study(api_base, workspace_id, actual_study_code, study_title)
                print(f"    –°–æ–∑–¥–∞–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: study_id={study_id}")
            
            if not document_id:
                # –°–æ–∑–¥–∞—ë–º –¥–æ–∫—É–º–µ–Ω—Ç
                doc_title = file_path.stem
                document_id = create_document(api_base, study_id, "protocol", doc_title)
                print(f"    –°–æ–∑–¥–∞–Ω –¥–æ–∫—É–º–µ–Ω—Ç: document_id={document_id}")
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –¥–ª—è effective_date
            file_mtime = os.path.getmtime(file_path)
            effective_date = datetime.fromtimestamp(file_mtime).date().isoformat()
            
            # –°–æ–∑–¥–∞—ë–º –≤–µ—Ä—Å–∏—é
            version_id = create_document_version(api_base, document_id, version_label, effective_date=effective_date)
            print(f"    –°–æ–∑–¥–∞–Ω–∞ –≤–µ—Ä—Å–∏—è: version_id={version_id} (effective_date={effective_date})")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
        print(f"    –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞...")
        upload_result = upload_file(api_base, version_id, file_path)
        print(f"    –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: sha256={upload_result.get('sha256', '')[:16]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –∏–Ω–≥–µ—Å—Ç–∏–∏
        version_data = get_version_status(api_base, version_id)
        current_status = version_data.get("ingestion_status")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω–≥–µ—Å—Ç–∏—é (–µ—Å–ª–∏ –æ–Ω–∞ –µ—â—ë –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è)
        if current_status == "processing":
            print(f"    –ò–Ω–≥–µ—Å—Ç–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è (—Å—Ç–∞—Ç—É—Å: {current_status})")
            print(f"    –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∏–Ω–≥–µ—Å—Ç–∏–∏...")
        else:
            print(f"    –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏–Ω–≥–µ—Å—Ç–∏–∏...")
            print(f"      - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ (Anchors)")
            print(f"      - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ Schedule of Activities (SoA)")
            print(f"      - –°–æ–∑–¥–∞–Ω–∏–µ Chunks (Narrative Index)")
            print(f"      - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ (Rules-first)")
            print(f"      - –ú–∞–ø–ø–∏–Ω–≥ —Å–µ–∫—Ü–∏–π (Section Mapping)")
            was_started, conflict_status = start_ingestion(api_base, version_id, force=True)
            
            if not was_started and conflict_status == "processing":
                print(f"    –ò–Ω–≥–µ—Å—Ç–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è, –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è...")
        
        # –û–∂–∏–¥–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        print(f"    –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∏–Ω–≥–µ—Å—Ç–∏–∏ (—Ç–∞–π–º–∞—É—Ç: {ingestion_timeout} —Å–µ–∫)...")
        final_status, version_data = poll_ingestion_status(api_base, version_id, ingestion_timeout)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        detailed_stats = extract_detailed_stats(version_data)
        summary = version_data.get("ingestion_summary_json", {})
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = {
            "version_id": version_id,
            "version_label": version_label,
            "final_status": final_status,
            "anchors_created": summary.get("anchors_created", 0),
            "chunks_created": summary.get("chunks_created", 0),
            "warnings": summary.get("warnings", []),
            "detailed": detailed_stats,
            "ingestion_summary": summary,
            "matched_anchors": summary.get("matched_anchors", 0),
            "changed_anchors": summary.get("changed_anchors", 0),
        }
        
        # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö —à–∞–≥–∞—Ö
        print(f"    –î–µ—Ç–∞–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
        print(f"      ‚úì Anchors —Å–æ–∑–¥–∞–Ω–æ: {stats['anchors_created']}")
        
        # SoA
        soa = detailed_stats["soa"]
        if soa["detected"]:
            print(f"      ‚úì SoA –∏–∑–≤–ª–µ—á—ë–Ω: {soa['visits_count']} –≤–∏–∑–∏—Ç–æ–≤, {soa['procedures_count']} –ø—Ä–æ—Ü–µ–¥—É—Ä")
            if soa["confidence"]:
                print(f"        (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {soa['confidence']:.2f})")
        else:
            print(f"      ‚ö† SoA –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω")
        
        # Chunks
        chunks = detailed_stats["chunks"]
        print(f"      ‚úì Chunks —Å–æ–∑–¥–∞–Ω–æ: {chunks['created']}")
        if chunks["anchors_per_chunk_avg"]:
            print(f"        (—Å—Ä–µ–¥–Ω–µ–µ anchors/chunk: {chunks['anchors_per_chunk_avg']})")
        
        # Facts
        facts = detailed_stats["facts"]
        print(f"      ‚úì –§–∞–∫—Ç–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–æ: {facts['total_extracted']}")
        if facts["needs_review"]:
            print(f"        (—Ç—Ä–µ–±—É—é—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏: {len(facts['needs_review'])})")
        
        # Section Mapping
        mapping = detailed_stats["section_mapping"]
        print(f"      ‚úì –°–µ–∫—Ü–∏–π —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ: {mapping['sections_mapped']}")
        if mapping["needs_review"] > 0:
            print(f"        (—Ç—Ä–µ–±—É—é—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏: {mapping['needs_review']})")
        
        # Topic Mapping Quality
        topics = detailed_stats.get("topics", {})
        if topics:
            topics_mapped = topics.get("mapped_count", 0)
            topics_total = topics.get("total_topics", 15)
            topics_rate = topics.get("mapped_rate", 0.0)
            print(f"      üìä Topic Mapping Quality: {topics_rate * 100:.1f}% ({topics_mapped}/{topics_total})")
        
        # Fact Extraction
        facts = detailed_stats.get("facts", {})
        facts_total = facts.get("total_extracted", 0)
        facts_conflicts = facts.get("conflicting_count", 0)
        print(f"      üß™ Fact Extraction: {facts_total} found, {facts_conflicts} conflicts.")
        
        # LLM –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è)
        llm_info = summary.get("llm_info")
        if llm_info:
            llm_model = llm_info.get("model", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
            llm_provider = llm_info.get("provider", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
            llm_prompt = llm_info.get("system_prompt")
            print(f"      ü§ñ LLM –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω:")
            print(f"        - –ú–æ–¥–µ–ª—å: {llm_model}")
            print(f"        - –ü—Ä–æ–≤–∞–π–¥–µ—Ä: {llm_provider}")
            if llm_prompt:
                # –û–±—Ä–µ–∑–∞–µ–º –ø—Ä–æ–º—Ç –¥–ª—è –≤—ã–≤–æ–¥–∞ (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤)
                prompt_preview = llm_prompt[:200].replace("\n", " ")
                if len(llm_prompt) > 200:
                    prompt_preview += "..."
                print(f"        - –ü—Ä–æ–º—Ç (–ø—Ä–µ–≤—å—é): {prompt_preview}")
        
        if final_status == "ready":
            print(f"    ‚úì –ò–Ω–≥–µ—Å—Ç–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return version_id, True, stats
        elif final_status == "needs_review":
            print(f"    ‚ö† –ò–Ω–≥–µ—Å—Ç–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞")
            return version_id, True, stats
        else:  # failed
            print(f"    ‚úó –ò–Ω–≥–µ—Å—Ç–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–æ–π")
            return version_id, False, stats
        
    except SystemExit:
        raise
    except Exception as e:
        print(f"    ‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
        import traceback
        traceback.print_exc()
        return version_id if version_id else "", False, {}


def process_study(
    study_code: str,
    files: List[Path],
    api_base: str,
    workspace_id: str,
    ingestion_timeout: int,
    project_root: Path,
) -> List[Dict[str, Any]]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª—ã –æ–¥–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ (–≤–µ—Ä—Å–∏–∏ –≤ –ø–æ—Ä—è–¥–∫–µ mtime)."""
    print(f"\n{'='*80}")
    print(f"–°–¢–ê–†–¢ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø: {study_code} (—Ñ–∞–π–ª–æ–≤: {len(files)})")
    print(f"{'='*80}")
    
    study_title = f"Benchmark: {study_code}"
    study_id = ensure_study(api_base, workspace_id, study_code, study_title)
    document_id = ensure_protocol_document(api_base, study_id, f"{study_code} Protocol")
    
    study_results: List[Dict[str, Any]] = []
    
    for idx, file_path in enumerate(files, 1):
        version_label = f"v{idx}.0"
        print(f"\n--- –í–µ—Ä—Å–∏—è {version_label} ({file_path.name}) ---")
        started_at = time.time()
        
        version_id_result, success, stats = process_file(
            api_base=api_base,
            workspace_id=workspace_id,
            file_path=file_path,
            study_code=study_code,
            study_id=study_id,
            document_id=document_id,
            version_id=None,
            create_new_study=False,
            version_label=version_label,
            ingestion_timeout=ingestion_timeout,
        )
        
        duration = round(time.time() - started_at, 2)
        stats = stats or {}
        stats["processing_time_sec"] = duration
        
        if success and version_id_result:
            save_benchmark_artifacts(
                api_base=api_base,
                project_root=project_root,
                study_code=study_code,
                version_number=idx,
                study_id=study_id,
                version_id=version_id_result,
            )
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º soa_confidence: —Å–Ω–∞—á–∞–ª–∞ –∏–∑ –∫–æ—Ä–Ω—è summary, –ø–æ—Ç–æ–º –∏–∑ detailed
        summary = stats.get("ingestion_summary", {})
        soa_conf = summary.get("soa_confidence")
        if soa_conf is None:
            soa_conf = (stats.get("detailed", {}) or {}).get("soa", {}).get("confidence")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ detailed_stats
        detailed = stats.get("detailed", {}) or {}
        topics = detailed.get("topics", {})
        facts = detailed.get("facts", {})
        
        topics_rate = topics.get("mapped_rate", 0.0)
        facts_total = facts.get("total_extracted", 0)
        facts_validated = facts.get("validated_count", 0)
        facts_conflicts = facts.get("conflicting_count", 0)
        
        study_results.append(
            {
                "study_code": study_code,
                "file_name": file_path.name,
                "version": version_label,
                "status": stats.get("final_status", "failed" if not success else "unknown"),
                "anchors_count": stats.get("anchors_created", 0),
                "soa_confidence": soa_conf,
                "matched_anchors": stats.get("matched_anchors", 0),
                "changed_anchors": stats.get("changed_anchors", 0),
                "topics_rate": topics_rate,
                "facts_total": facts_total,
                "facts_validated": facts_validated,
                "facts_conflicts": facts_conflicts,
                "processing_time_sec": duration,
                "version_id": version_id_result,
                "success": success,
                "stats": stats,
            }
        )
    
    return study_results


def main() -> None:
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è Windows
    try:
        sys.stdout.reconfigure(encoding="utf-8")
        sys.stderr.reconfigure(encoding="utf-8")
    except Exception:
        pass
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    if load_dotenv:
        script_dir = Path(__file__).parent.absolute()
        backend_dir = script_dir.parent / "backend"
        env_path = backend_dir / ".env"
        
        if env_path.exists():
            load_dotenv(env_path, override=False)
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ {env_path}")
    
    parser = argparse.ArgumentParser(
        description="–ú–∞—Å—Å–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –∏–Ω–≥–µ—Å—Ç–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω —Ñ–∞–π–ª
  python batch_upload_ingest.py file.docx --workspace-id <UUID> --api http://localhost:8000
  
  # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ
  python batch_upload_ingest.py ./documents --workspace-id <UUID>
  
  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ study/document/version
  python batch_upload_ingest.py file.docx --version-id <UUID>
  
  # –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É —Å –ø—Ä–æ–ø—É—Å–∫–æ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
  python batch_upload_ingest.py ./documents --workspace-id <UUID> --resume
        """
    )
    
    parser.add_argument(
        "path",
        type=str,
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ –ø–∞–ø–∫–µ —Å —Ñ–∞–π–ª–∞–º–∏"
    )
    parser.add_argument(
        "--api",
        default="http://localhost:8000",
        help="–ë–∞–∑–æ–≤—ã–π URL API (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: http://localhost:8000)"
    )
    parser.add_argument(
        "--workspace-id",
        default="",
        help="UUID —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω --version-id)"
    )
    parser.add_argument(
        "--version-id",
        default="",
        help="UUID —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –≤–µ—Ä—Å–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω, —Ñ–∞–π–ª –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –≤ —ç—Ç—É –≤–µ—Ä—Å–∏—é)"
    )
    parser.add_argument(
        "--study-id",
        default="",
        help="UUID —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"
    )
    parser.add_argument(
        "--document-id",
        default="",
        help="UUID —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=1200,
        help="–¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –∏–Ω–≥–µ—Å—Ç–∏–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 600)"
    )
    parser.add_argument(
        "--resume",
        action="store_true",
        help="–ü—Ä–æ–ø—É—Å–∫–∞—Ç—å —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (–ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ SHA256 –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö) –∏ –Ω–∞—á–∏–Ω–∞—Ç—å —Å –ø–µ—Ä–≤–æ–≥–æ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ"
    )
    parser.add_argument(
        "--max-studies",
        type=int,
        default=0,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π (0 ‚Äî –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="–í—ã–≤–æ–¥–∏—Ç—å –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (—Ö–µ—à–∏ —Ñ–∞–π–ª–æ–≤, —Å—Ç–∞—Ç—É—Å—ã –∏ —Ç.–¥.)"
    )
    
    args = parser.parse_args()
    
    api_base = args.api.rstrip("/")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ workspace_id
    workspace_id = args.workspace_id.strip()
    version_id = args.version_id.strip()
    
    if not version_id:
        if not workspace_id:
            workspace_id = input("–í–≤–µ–¥–∏—Ç–µ WORKSPACE_ID (UUID): ").strip()
        if len(workspace_id) != 36:
            die(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç WORKSPACE_ID (–æ–∂–∏–¥–∞–µ—Ç—Å—è UUID –∏–∑ 36 —Å–∏–º–≤–æ–ª–æ–≤): {workspace_id}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å
    input_path = Path(args.path)
    if not input_path.is_absolute():
        # –ï—Å–ª–∏ –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π, –¥–µ–ª–∞–µ–º –µ–≥–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        input_path = Path.cwd() / input_path
    
    project_root = Path(__file__).resolve().parent.parent
    
    study_files = discover_study_files(input_path)
    if not study_files:
        die(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤: {input_path}")
    
    processed_hashes: Set[str] = set()
    skipped_count = 0
    
    if args.resume:
        if not workspace_id:
            die("–î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è --resume –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å --workspace-id")
        print(f"\n{'='*80}")
        print("–ü–†–û–í–ï–†–ö–ê –û–ë–†–ê–ë–û–¢–ê–ù–ù–´–• –§–ê–ô–õ–û–í")
        print(f"{'='*80}")
        print(f"–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        processed_hashes = get_processed_sha256_set(api_base, workspace_id, debug=args.debug)
        study_files, skipped_count = filter_processed_by_hash(study_files, processed_hashes)
        if not study_files:
            print("–í—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã (ready/needs_review). –í—ã—Ö–æ–¥.")
            sys.exit(0)
    
    study_items = sorted(study_files.items(), key=lambda x: x[0])
    if args.max_studies and args.max_studies > 0:
        if args.max_studies < len(study_items):
            print(f"\n–ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã—Ö {args.max_studies} –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏–∑ {len(study_items)} (–ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É).")
        study_items = study_items[:args.max_studies]
    
    total_files = sum(len(v) for _, v in study_items)
    print(f"\n{'='*80}")
    print(f"–ù–ê–ô–î–ï–ù–´ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø: {len(study_items)} (—Ñ–∞–π–ª–æ–≤: {total_files})")
    if skipped_count:
        print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –ø–æ --resume: {skipped_count}")
    print(f"{'='*80}")
    for study_code, files in study_items:
        print(f"  ‚Ä¢ {study_code}: {len(files)} —Ñ–∞–π–ª(–æ–≤)")
        for idx, file_path in enumerate(files, 1):
            print(f"      {idx}. {file_path.name}")
    print()
    
    all_rows: List[Dict[str, Any]] = []
    
    for study_code, files in study_items:
        try:
            rows = process_study(
                study_code,
                files,
                api_base,
                workspace_id,
                args.timeout,
                project_root,
            )
            all_rows.extend(rows)
        except Exception as e:
            print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è {study_code}: {e}")
    
    # –ó–∞–ø–∏—Å—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞
    summary_path = project_root / "benchmark_summary.csv"
    ensure_dir(summary_path.parent)
    fieldnames = [
        "study_code",
        "file_name",
        "version",
        "status",
        "anchors_count",
        "soa_confidence",
        "matched_anchors",
        "changed_anchors",
        "topics_rate",
        "facts_total",
        "facts_validated",
        "facts_conflicts",
        "processing_time_sec",
    ]
    with open(summary_path, "w", encoding="utf-8", newline="") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for row in all_rows:
            writer.writerow({k: row.get(k, "") for k in fieldnames})
    
    total_versions = len(all_rows)
    successful = sum(1 for r in all_rows if r.get("success"))
    failed = total_versions - successful
    
    print(f"\n{'='*80}")
    print("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print(f"{'='*80}")
    print(f"–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(study_items)}")
    print(f"–í—Å–µ–≥–æ –≤–µ—Ä—Å–∏–π: {total_versions}")
    print(f"–£—Å–ø–µ—à–Ω–æ (ready/needs_review): {successful}")
    print(f"–° –æ—à–∏–±–∫–∞–º–∏: {failed}")
    print(f"CSV –æ—Ç—á—ë—Ç: {summary_path}")
    
    if all_rows:
        print(f"\n{'='*80}")
        print("–î–ï–¢–ê–õ–ò –ü–û –í–ï–†–°–ò–Ø–ú:")
        print(f"{'='*80}")
        for row in all_rows:
            status_icon = "‚úì" if row.get("success") else "‚úó"
            print(f"\n{status_icon} {row['study_code']} {row['version']} ({row['file_name']}): {row.get('status')}")
            print(f"  Anchors: {row.get('anchors_count', 0)}")
            print(f"  SoA confidence: {row.get('soa_confidence')}")
            print(f"  matched_anchors: {row.get('matched_anchors', 0)}, changed_anchors: {row.get('changed_anchors', 0)}")
            topics_rate = row.get('topics_rate')
            if topics_rate is not None:
                print(f"  Topics rate: {topics_rate}%")
            print(f"  Facts: {row.get('facts_total', 0)} total, {row.get('facts_validated', 0)} validated, {row.get('facts_conflicts', 0)} conflicts")
            print(f"  processing_time_sec: {row.get('processing_time_sec')}")
    
    if failed > 0:
        sys.exit(1)
    else:
        print(f"\n{'='*80}")
        print("–í–°–ï –í–ï–†–°–ò–ò –û–ë–†–ê–ë–û–¢–ê–ù–´ (–±–µ–∑ –æ—à–∏–±–æ–∫ –≤ —Å—Ç–∞—Ç—É—Å–µ ready/needs_review)")
        print(f"{'='*80}")


if __name__ == "__main__":
    main()

