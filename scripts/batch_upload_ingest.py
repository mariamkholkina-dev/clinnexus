#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –º–∞—Å—Å–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∏–Ω–≥–µ—Å—Ç–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–ü—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥:
  - –ø—É—Ç—å –∫ –æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É (.docx, .pdf, .xlsx)
  - –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ñ–∞–π–ª–∞–º–∏

–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: API –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ .docx, .pdf, .xlsx. 
–§–∞–π–ª—ã —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º .doc (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç Word) –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º.

–î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞:
  1. –°–æ–∑–¥–∞—ë—Ç study/document/version (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
  2. –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª —á–µ—Ä–µ–∑ upload API
  3. –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –∏–Ω–≥–µ—Å—Ç–∏–∏
  4. –û–∂–∏–¥–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∏–Ω–≥–µ—Å—Ç–∏–∏

–í –∫–æ–Ω—Ü–µ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º.

–û–ø—Ü–∏—è --resume:
  –ü–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (–ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ SHA256 –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö)
  –∏ –Ω–∞—á–∏–Ω–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É —Å –ø–µ—Ä–≤–æ–≥–æ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞. –ü–æ–ª–µ–∑–Ω–æ –ø—Ä–∏ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–∏
  –º–∞—Å—Å–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ - –º–æ–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –º–µ—Å—Ç–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.
"""

import argparse
import hashlib
import json
import os
import sys
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

try:
    import requests
except ImportError:
    print("–û–®–ò–ë–ö–ê: –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ requests. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install requests")
    sys.exit(1)

try:
    from dotenv import load_dotenv
except ImportError:
    load_dotenv = None


class HTTPError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è HTTP –æ—à–∏–±–æ–∫."""
    pass


def die(msg: str, code: int = 1) -> None:
    """–ó–∞–≤–µ—Ä—à–∞–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å –æ—à–∏–±–∫–æ–π."""
    print(f"–û–®–ò–ë–ö–ê: {msg}", file=sys.stderr)
    sys.exit(code)


def http_json(method: str, url: str, *, json_body: Any = None, params: Dict[str, Any] | None = None, timeout: int = 30, raise_on_error: bool = True) -> Any:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç HTTP –∑–∞–ø—Ä–æ—Å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON –æ—Ç–≤–µ—Ç."""
    headers = {"Accept": "application/json"}
    if json_body is not None:
        headers["Content-Type"] = "application/json"
    r = requests.request(method, url, headers=headers, json=json_body, params=params, timeout=timeout)
    if r.status_code >= 400:
        try:
            body = r.json()
            error_msg = json.dumps(body, indent=2, ensure_ascii=False)
        except Exception:
            error_msg = r.text
        error_text = f"{method} {url} -> {r.status_code}\n{error_msg}"
        if raise_on_error:
            die(error_text)
        else:
            raise HTTPError(error_text)
    if r.text.strip() == "":
        return None
    return r.json()


def upload_file(api_base: str, version_id: str, file_path: Path) -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª –¥–ª—è –≤–µ—Ä—Å–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
    url = f"{api_base}/api/document-versions/{version_id}/upload"
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º content-type –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
    ext = file_path.suffix.lower()
    content_types = {
        ".doc": "application/msword",
        ".docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        ".pdf": "application/pdf",
        ".xlsx": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    }
    content_type = content_types.get(ext, "application/octet-stream")
    
    with open(file_path, "rb") as f:
        files = {"file": (file_path.name, f, content_type)}
        r = requests.post(url, files=files, headers={"Accept": "application/json"}, timeout=300)
    
    if r.status_code >= 400:
        die(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {r.status_code}: {r.text}")
    
    return r.json()


def get_version_status(api_base: str, version_id: str) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å –≤–µ—Ä—Å–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
    url = f"{api_base}/api/document-versions/{version_id}"
    return http_json("GET", url, timeout=30)


def start_ingestion(api_base: str, version_id: str, force: bool = True) -> Tuple[bool, Optional[str]]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –∏–Ω–≥–µ—Å—Ç–∏–∏ –¥–ª—è –≤–µ—Ä—Å–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        (was_started, current_status) - –±—ã–ª–∞ –ª–∏ –∏–Ω–≥–µ—Å—Ç–∏—è –∑–∞–ø—É—â–µ–Ω–∞ –∏ —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å
        –ï—Å–ª–∏ –∏–Ω–≥–µ—Å—Ç–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (False, "processing")
    """
    url = f"{api_base}/api/document-versions/{version_id}/ingest"
    if force:
        url += "?force=true"
    
    try:
        # –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–µ—Ä–≤–µ—Ä—ã —Ç—Ä–µ–±—É—é—Ç –ø—É—Å—Ç–æ–µ —Ç–µ–ª–æ –∏–ª–∏ –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç –µ–≥–æ
        r = requests.post(url, json={}, headers={"Accept": "application/json"}, timeout=120)
    except Exception:
        # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –±–µ–∑ json
        r = requests.post(url, headers={"Accept": "application/json"}, timeout=120)
    
    if r.status_code == 409:
        # –ö–æ–Ω—Ñ–ª–∏–∫—Ç: –∏–Ω–≥–µ—Å—Ç–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è
        try:
            error_body = r.json()
            current_status = error_body.get("details", {}).get("current_status", "processing")
            return False, current_status
        except Exception:
            return False, "processing"
    
    if r.status_code >= 400:
        die(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∏–Ω–≥–µ—Å—Ç–∏–∏ {r.status_code}: {r.text}")
    
    return True, None


def poll_ingestion_status(
    api_base: str, version_id: str, timeout_sec: int = 600
) -> Tuple[str, Dict[str, Any]]:
    """
    –û–∂–∏–¥–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∏–Ω–≥–µ—Å—Ç–∏–∏, –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä—è—è —Å—Ç–∞—Ç—É—Å.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        (final_status, version_data)
    """
    deadline = time.time() + timeout_sec
    url = f"{api_base}/api/document-versions/{version_id}"
    
    last_status = None
    while True:
        try:
            v = http_json("GET", url, timeout=30)
            status = v.get("ingestion_status")
            
            # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç—É—Å, –µ—Å–ª–∏ –æ–Ω –∏–∑–º–µ–Ω–∏–ª—Å—è
            if status != last_status:
                print(f"    –°—Ç–∞—Ç—É—Å –∏–Ω–≥–µ—Å—Ç–∏–∏: {status}")
                last_status = status
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—É—Å—ã
            if status in ("ready", "needs_review", "failed"):
                return status, v
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞–π–º–∞—É—Ç
            if time.time() > deadline:
                die(f"–¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∏–Ω–≥–µ—Å—Ç–∏–∏ (>{timeout_sec} —Å–µ–∫)")
            
            time.sleep(2)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∫–∞–∂–¥—ã–µ 2 —Å–µ–∫—É–Ω–¥—ã
            
        except KeyboardInterrupt:
            print("\n–ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            sys.exit(1)
        except Exception as e:
            print(f"    –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å—Ç–∞—Ç—É—Å–∞: {e}")
            time.sleep(2)


def create_study(api_base: str, workspace_id: str, study_code: str, title: str) -> str:
    """–°–æ–∑–¥–∞—ë—Ç –Ω–æ–≤–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç study_id."""
    url = f"{api_base}/api/studies"
    body = {
        "workspace_id": workspace_id,
        "study_code": study_code,
        "title": title,
        "status": "active",
    }
    study = http_json("POST", url, json_body=body, timeout=30)
    return study["id"]


def create_document(api_base: str, study_id: str, doc_type: str, title: str) -> str:
    """–°–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç document_id."""
    url = f"{api_base}/api/studies/{study_id}/documents"
    body = {
        "doc_type": doc_type,
        "title": title,
        "lifecycle_status": "draft",
    }
    doc = http_json("POST", url, json_body=body, timeout=30)
    return doc["id"]


def create_document_version(api_base: str, document_id: str, version_label: str) -> str:
    """–°–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç version_id."""
    url = f"{api_base}/api/documents/{document_id}/versions"
    body = {"version_label": version_label}
    ver = http_json("POST", url, json_body=body, timeout=30)
    return ver["id"]


def extract_detailed_stats(version_data: Dict[str, Any]) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —à–∞–≥–∞–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏–∏."""
    summary = version_data.get("ingestion_summary_json", {})
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ SoA (Schedule of Activities)
    soa_stats = {
        "detected": False,
        "confidence": None,
        "visits_count": 0,
        "procedures_count": 0,
        "matrix_cells": 0,
    }
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ SoA —Ñ–∞–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ ingestion_summary_json
    soa_facts = summary.get("soa_facts_written", {})
    if isinstance(soa_facts, dict):
        soa_stats["detected"] = (
            soa_facts.get("visits", False) or
            soa_facts.get("procedures", False) or
            soa_facts.get("matrix", False)
        )
        counts = soa_facts.get("counts", {})
        soa_stats["visits_count"] = counts.get("visits", 0)
        soa_stats["procedures_count"] = counts.get("procedures", 0)
        soa_stats["matrix_cells"] = counts.get("matrix", 0)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞–∫–∂–µ —á–µ—Ä–µ–∑ –º–µ—Ç—Ä–∏–∫–∏
    metrics = summary.get("metrics", {})
    soa_metrics = metrics.get("soa", {})
    if soa_metrics.get("found"):
        soa_stats["detected"] = True
        soa_stats["confidence"] = soa_metrics.get("table_score")
        if soa_stats["visits_count"] == 0:
            soa_stats["visits_count"] = soa_metrics.get("visits_count", 0)
        if soa_stats["procedures_count"] == 0:
            soa_stats["procedures_count"] = soa_metrics.get("procedures_count", 0)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ Chunks (Narrative Index)
    chunks_stats = {
        "created": summary.get("chunks_created", 0),
        "anchors_per_chunk_avg": None,
    }
    chunk_metrics = metrics.get("chunks", {})
    if chunk_metrics:
        chunks_stats["created"] = chunk_metrics.get("total", chunks_stats["created"])
        anchors_count = summary.get("anchors_created", 0)
        if chunks_stats["created"] > 0 and anchors_count > 0:
            chunks_stats["anchors_per_chunk_avg"] = round(anchors_count / chunks_stats["created"], 2)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ Facts (Rules-first)
    facts_stats = {
        "total_extracted": summary.get("facts_count", 0),
        "needs_review": [],
        "by_type": {},
    }
    facts_metrics = metrics.get("facts", {})
    if facts_metrics:
        facts_stats["total_extracted"] = facts_metrics.get("total", facts_stats["total_extracted"])
        facts_stats["needs_review"] = facts_metrics.get("needs_review_list", [])
        facts_stats["by_type"] = facts_metrics.get("by_type", {})
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ Section Mapping
    mapping_stats = {
        "sections_mapped": summary.get("sections_mapped_count", 0),
        "needs_review": summary.get("sections_needs_review_count", 0),
        "warnings": summary.get("mapping_warnings", []),
    }
    
    # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ docx_summary, –µ—Å–ª–∏ –µ—Å—Ç—å
    docx_summary = summary.get("docx_summary") or {}
    if docx_summary:
        if mapping_stats["sections_mapped"] == 0:
            mapping_stats["sections_mapped"] = docx_summary.get("sections_mapped_count", 0)
        if mapping_stats["needs_review"] == 0:
            mapping_stats["needs_review"] = docx_summary.get("sections_needs_review_count", 0)
        if not mapping_stats["warnings"]:
            mapping_stats["warnings"] = docx_summary.get("mapping_warnings", [])
    
    return {
        "soa": soa_stats,
        "chunks": chunks_stats,
        "facts": facts_stats,
        "section_mapping": mapping_stats,
    }


def calculate_file_sha256(file_path: Path) -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç SHA256 —Ö–µ—à —Ñ–∞–π–ª–∞."""
    sha256_hash = hashlib.sha256()
    with open(file_path, "rb") as f:
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –±–ª–æ–∫–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()


def get_processed_sha256_set(api_base: str, workspace_id: str, debug: bool = False) -> Set[str]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ SHA256 —Ö–µ—à–µ–π –≤—Å–µ—Ö –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç set —Å SHA256 —Ö–µ—à–∞–º–∏ (–≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è).
    """
    processed_hashes: Set[str] = set()
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ workspace
        url = f"{api_base}/api/studies"
        params = {"workspace_id": workspace_id}
        
        try:
            studies = http_json("GET", url, params=params, timeout=60, raise_on_error=False)
        except Exception as e:
            # –ü–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ–º HTTPError –∏ –¥—Ä—É–≥–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
            print(f"    –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏–∑ API: {e}")
            print(f"    URL: {url}")
            print(f"    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {params}")
            print(f"    –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
            return processed_hashes
        
        if not isinstance(studies, list):
            print(f"    –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π")
            return processed_hashes
        
        print(f"    –ù–∞–π–¥–µ–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π: {len(studies)}")
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        for study in studies:
            study_id = study.get("id")
            if not study_id:
                continue
            
            try:
                # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
                docs_url = f"{api_base}/api/studies/{study_id}/documents"
                documents = http_json("GET", docs_url, timeout=60)
                
                if not isinstance(documents, list):
                    continue
                
                # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ–ª—É—á–∞–µ–º –≤–µ—Ä—Å–∏–∏
                for doc in documents:
                    doc_id = doc.get("id")
                    if not doc_id:
                        continue
                    
                    try:
                        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä—Å–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                        versions_url = f"{api_base}/api/documents/{doc_id}/versions"
                        versions = http_json("GET", versions_url, timeout=60)
                        
                        if not isinstance(versions, list):
                            continue
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º SHA256 –∏–∑ –≤–µ—Ä—Å–∏–π
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã —Å –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–π –∏–Ω–≥–µ—Å—Ç–∏–µ–π (ready –∏–ª–∏ needs_review)
                        for version in versions:
                            sha256 = version.get("source_sha256")
                            ingestion_status = version.get("ingestion_status")
                            
                            if debug:
                                version_id = version.get("id", "unknown")
                                print(f"      –í–µ—Ä—Å–∏—è {version_id}: SHA256={sha256[:16] if sha256 else 'None'}..., —Å—Ç–∞—Ç—É—Å={ingestion_status}")
                            
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã —Å —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–π –∏–Ω–≥–µ—Å—Ç–∏–µ–π
                            # (ready –∏–ª–∏ needs_review). –§–∞–π–ª—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º failed –∏–ª–∏ processing
                            # –±—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∑–∞–Ω–æ–≤–æ
                            if sha256 and ingestion_status in ("ready", "needs_review"):
                                processed_hashes.add(sha256.lower())
                                if debug:
                                    print(f"        -> –î–æ–±–∞–≤–ª–µ–Ω –≤ —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö")
                            elif debug and sha256:
                                print(f"        -> –ü—Ä–æ–ø—É—â–µ–Ω (—Å—Ç–∞—Ç—É—Å {ingestion_status}, –Ω–µ ready/needs_review)")
                    
                    except Exception as e:
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≤–µ—Ä—Å–∏–π –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                        continue
            
            except Exception as e:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
                continue
        
        print(f"    –ù–∞–π–¥–µ–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (ready/needs_review): {len(processed_hashes)}")
        if debug and processed_hashes:
            print(f"    –ü—Ä–∏–º–µ—Ä—ã —Ö–µ—à–µ–π –∏–∑ –±–∞–∑—ã (–ø–µ—Ä–≤—ã–µ 5):")
            for i, h in enumerate(list(processed_hashes)[:5], 1):
                print(f"      {i}. {h[:32]}...")
        return processed_hashes
    
    except Exception as e:
        print(f"    –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {e}")
        print(f"    –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
        return processed_hashes


def find_document_files(path: Path) -> List[Path]:
    """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –ø—É—Ç–∏ (—Ñ–∞–π–ª –∏–ª–∏ –ø–∞–ø–∫–∞)."""
    # API –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ .docx, .pdf, .xlsx (–Ω–µ .doc)
    allowed_extensions = {".docx", ".pdf", ".xlsx"}
    
    if path.is_file():
        ext = path.suffix.lower()
        if ext == ".doc":
            print(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –§–∞–π–ª {path.name} –∏–º–µ–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ .doc (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç Word).")
            print(f"  API –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ .docx, .pdf, .xlsx. –§–∞–π–ª –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω.")
            print(f"  –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª –≤ .docx –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π.")
            return []
        if ext in allowed_extensions:
            return [path]
        else:
            die(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {ext}. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: {', '.join(allowed_extensions)}")
    
    if path.is_dir():
        files = []
        skipped_doc = []
        for ext in allowed_extensions:
            files.extend(path.glob(f"**/*{ext}"))
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º .doc —Ñ–∞–π–ª—ã –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        for doc_file in path.glob("**/*.doc"):
            if doc_file.is_file():
                skipped_doc.append(doc_file)
        if skipped_doc:
            print(f"\n–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ù–∞–π–¥–µ–Ω–æ {len(skipped_doc)} —Ñ–∞–π–ª(–æ–≤) —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º .doc (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç Word):")
            for doc_file in skipped_doc[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                print(f"  - {doc_file}")
            if len(skipped_doc) > 10:
                print(f"  ... –∏ –µ—â—ë {len(skipped_doc) - 10} —Ñ–∞–π–ª(–æ–≤)")
            print(f"  API –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ .docx, .pdf, .xlsx. –≠—Ç–∏ —Ñ–∞–π–ª—ã –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã.")
            print(f"  –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª—ã –≤ .docx –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π.\n")
        return sorted(files)
    
    die(f"–ü—É—Ç—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {path}")


def extract_detailed_stats(version_data: Dict[str, Any]) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —à–∞–≥–∞–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏–∏."""
    summary = version_data.get("ingestion_summary_json", {})
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ SoA (Schedule of Activities)
    soa_stats = {
        "detected": False,
        "confidence": None,
        "visits_count": 0,
        "procedures_count": 0,
        "matrix_cells": 0,
    }
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ SoA —Ñ–∞–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ ingestion_summary_json
    soa_facts = summary.get("soa_facts_written", {})
    if isinstance(soa_facts, dict):
        soa_stats["detected"] = (
            soa_facts.get("visits", False) or
            soa_facts.get("procedures", False) or
            soa_facts.get("matrix", False)
        )
        counts = soa_facts.get("counts", {})
        soa_stats["visits_count"] = counts.get("visits", 0)
        soa_stats["procedures_count"] = counts.get("procedures", 0)
        soa_stats["matrix_cells"] = counts.get("matrix", 0)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞–∫–∂–µ —á–µ—Ä–µ–∑ –º–µ—Ç—Ä–∏–∫–∏
    metrics = summary.get("metrics", {})
    soa_metrics = metrics.get("soa", {})
    if soa_metrics.get("found"):
        soa_stats["detected"] = True
        soa_stats["confidence"] = soa_metrics.get("table_score")
        if soa_stats["visits_count"] == 0:
            soa_stats["visits_count"] = soa_metrics.get("visits_count", 0)
        if soa_stats["procedures_count"] == 0:
            soa_stats["procedures_count"] = soa_metrics.get("procedures_count", 0)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ Chunks (Narrative Index)
    chunks_stats = {
        "created": summary.get("chunks_created", 0),
        "anchors_per_chunk_avg": None,
    }
    chunk_metrics = metrics.get("chunks", {})
    if chunk_metrics:
        chunks_stats["created"] = chunk_metrics.get("total", chunks_stats["created"])
        anchors_count = summary.get("anchors_created", 0)
        if chunks_stats["created"] > 0 and anchors_count > 0:
            chunks_stats["anchors_per_chunk_avg"] = round(anchors_count / chunks_stats["created"], 2)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ Facts (Rules-first)
    facts_stats = {
        "total_extracted": summary.get("facts_count", 0),
        "needs_review": [],
        "by_type": {},
    }
    facts_metrics = metrics.get("facts", {})
    if facts_metrics:
        facts_stats["total_extracted"] = facts_metrics.get("total", facts_stats["total_extracted"])
        facts_stats["needs_review"] = facts_metrics.get("needs_review_list", [])
        facts_stats["by_type"] = facts_metrics.get("by_type", {})
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ Section Mapping
    mapping_stats = {
        "sections_mapped": summary.get("sections_mapped_count", 0),
        "needs_review": summary.get("sections_needs_review_count", 0),
        "warnings": summary.get("mapping_warnings", []),
    }
    
    # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ docx_summary, –µ—Å–ª–∏ –µ—Å—Ç—å
    docx_summary = summary.get("docx_summary") or {}
    if docx_summary:
        if mapping_stats["sections_mapped"] == 0:
            mapping_stats["sections_mapped"] = docx_summary.get("sections_mapped_count", 0)
        if mapping_stats["needs_review"] == 0:
            mapping_stats["needs_review"] = docx_summary.get("sections_needs_review_count", 0)
        if not mapping_stats["warnings"]:
            mapping_stats["warnings"] = docx_summary.get("mapping_warnings", [])
    
    return {
        "soa": soa_stats,
        "chunks": chunks_stats,
        "facts": facts_stats,
        "section_mapping": mapping_stats,
    }


def process_file(
    api_base: str,
    workspace_id: str,
    file_path: Path,
    study_id: Optional[str] = None,
    document_id: Optional[str] = None,
    version_id: Optional[str] = None,
    create_new_study: bool = True,
    ingestion_timeout: int = 600,
) -> Tuple[str, bool, Dict[str, Any]]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª: —Å–æ–∑–¥–∞—ë—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ), –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –∏–Ω–≥–µ—Å—Ç–∏—é.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        (version_id, success, stats)
    """
    file_name = file_path.name
    
    try:
        # –°–æ–∑–¥–∞—ë–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞
        if not version_id:
            if not study_id or create_new_study:
                # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
                study_code = f"BATCH-{int(time.time())}-{file_path.stem[:30]}"
                study_title = f"Batch Upload: {file_path.stem}"
                study_id = create_study(api_base, workspace_id, study_code, study_title)
                print(f"    –°–æ–∑–¥–∞–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: study_id={study_id}")
            
            if not document_id:
                # –°–æ–∑–¥–∞—ë–º –¥–æ–∫—É–º–µ–Ω—Ç
                doc_title = file_path.stem
                document_id = create_document(api_base, study_id, "protocol", doc_title)
                print(f"    –°–æ–∑–¥–∞–Ω –¥–æ–∫—É–º–µ–Ω—Ç: document_id={document_id}")
            
            # –°–æ–∑–¥–∞—ë–º –≤–µ—Ä—Å–∏—é
            version_label = "v1.0"
            version_id = create_document_version(api_base, document_id, version_label)
            print(f"    –°–æ–∑–¥–∞–Ω–∞ –≤–µ—Ä—Å–∏—è: version_id={version_id}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
        print(f"    –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞...")
        upload_result = upload_file(api_base, version_id, file_path)
        print(f"    –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: sha256={upload_result.get('sha256', '')[:16]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –∏–Ω–≥–µ—Å—Ç–∏–∏
        version_data = get_version_status(api_base, version_id)
        current_status = version_data.get("ingestion_status")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω–≥–µ—Å—Ç–∏—é (–µ—Å–ª–∏ –æ–Ω–∞ –µ—â—ë –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è)
        if current_status == "processing":
            print(f"    –ò–Ω–≥–µ—Å—Ç–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è (—Å—Ç–∞—Ç—É—Å: {current_status})")
            print(f"    –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∏–Ω–≥–µ—Å—Ç–∏–∏...")
        else:
            print(f"    –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏–Ω–≥–µ—Å—Ç–∏–∏...")
            print(f"      - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ (Anchors)")
            print(f"      - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ Schedule of Activities (SoA)")
            print(f"      - –°–æ–∑–¥–∞–Ω–∏–µ Chunks (Narrative Index)")
            print(f"      - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ (Rules-first)")
            print(f"      - –ú–∞–ø–ø–∏–Ω–≥ —Å–µ–∫—Ü–∏–π (Section Mapping)")
            was_started, conflict_status = start_ingestion(api_base, version_id, force=True)
            
            if not was_started and conflict_status == "processing":
                print(f"    –ò–Ω–≥–µ—Å—Ç–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è, –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è...")
        
        # –û–∂–∏–¥–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        print(f"    –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∏–Ω–≥–µ—Å—Ç–∏–∏ (—Ç–∞–π–º–∞—É—Ç: {ingestion_timeout} —Å–µ–∫)...")
        final_status, version_data = poll_ingestion_status(api_base, version_id, ingestion_timeout)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        detailed_stats = extract_detailed_stats(version_data)
        summary = version_data.get("ingestion_summary_json", {})
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = {
            "version_id": version_id,
            "final_status": final_status,
            "anchors_created": summary.get("anchors_created", 0),
            "chunks_created": summary.get("chunks_created", 0),
            "warnings": summary.get("warnings", []),
            "detailed": detailed_stats,
        }
        
        # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö —à–∞–≥–∞—Ö
        print(f"    –î–µ—Ç–∞–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
        print(f"      ‚úì Anchors —Å–æ–∑–¥–∞–Ω–æ: {stats['anchors_created']}")
        
        # SoA
        soa = detailed_stats["soa"]
        if soa["detected"]:
            print(f"      ‚úì SoA –∏–∑–≤–ª–µ—á—ë–Ω: {soa['visits_count']} –≤–∏–∑–∏—Ç–æ–≤, {soa['procedures_count']} –ø—Ä–æ—Ü–µ–¥—É—Ä")
            if soa["confidence"]:
                print(f"        (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {soa['confidence']:.2f})")
        else:
            print(f"      ‚ö† SoA –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω")
        
        # Chunks
        chunks = detailed_stats["chunks"]
        print(f"      ‚úì Chunks —Å–æ–∑–¥–∞–Ω–æ: {chunks['created']}")
        if chunks["anchors_per_chunk_avg"]:
            print(f"        (—Å—Ä–µ–¥–Ω–µ–µ anchors/chunk: {chunks['anchors_per_chunk_avg']})")
        
        # Facts
        facts = detailed_stats["facts"]
        print(f"      ‚úì –§–∞–∫—Ç–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–æ: {facts['total_extracted']}")
        if facts["needs_review"]:
            print(f"        (—Ç—Ä–µ–±—É—é—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏: {len(facts['needs_review'])})")
        
        # Section Mapping
        mapping = detailed_stats["section_mapping"]
        print(f"      ‚úì –°–µ–∫—Ü–∏–π —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ: {mapping['sections_mapped']}")
        if mapping["needs_review"] > 0:
            print(f"        (—Ç—Ä–µ–±—É—é—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏: {mapping['needs_review']})")
        
        if final_status == "ready":
            print(f"    ‚úì –ò–Ω–≥–µ—Å—Ç–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return version_id, True, stats
        elif final_status == "needs_review":
            print(f"    ‚ö† –ò–Ω–≥–µ—Å—Ç–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞")
            return version_id, True, stats
        else:  # failed
            print(f"    ‚úó –ò–Ω–≥–µ—Å—Ç–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–æ–π")
            return version_id, False, stats
        
    except SystemExit:
        raise
    except Exception as e:
        print(f"    ‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
        import traceback
        traceback.print_exc()
        return version_id if version_id else "", False, {}


def main() -> None:
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è Windows
    try:
        sys.stdout.reconfigure(encoding="utf-8")
        sys.stderr.reconfigure(encoding="utf-8")
    except Exception:
        pass
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    if load_dotenv:
        script_dir = Path(__file__).parent.absolute()
        backend_dir = script_dir.parent / "backend"
        env_path = backend_dir / ".env"
        
        if env_path.exists():
            load_dotenv(env_path, override=False)
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ {env_path}")
    
    parser = argparse.ArgumentParser(
        description="–ú–∞—Å—Å–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –∏–Ω–≥–µ—Å—Ç–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω —Ñ–∞–π–ª
  python batch_upload_ingest.py file.docx --workspace-id <UUID> --api http://localhost:8000
  
  # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ
  python batch_upload_ingest.py ./documents --workspace-id <UUID>
  
  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ study/document/version
  python batch_upload_ingest.py file.docx --version-id <UUID>
  
  # –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É —Å –ø—Ä–æ–ø—É—Å–∫–æ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
  python batch_upload_ingest.py ./documents --workspace-id <UUID> --resume
        """
    )
    
    parser.add_argument(
        "path",
        type=str,
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ –ø–∞–ø–∫–µ —Å —Ñ–∞–π–ª–∞–º–∏"
    )
    parser.add_argument(
        "--api",
        default="http://localhost:8000",
        help="–ë–∞–∑–æ–≤—ã–π URL API (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: http://localhost:8000)"
    )
    parser.add_argument(
        "--workspace-id",
        default="",
        help="UUID —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω --version-id)"
    )
    parser.add_argument(
        "--version-id",
        default="",
        help="UUID —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –≤–µ—Ä—Å–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω, —Ñ–∞–π–ª –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –≤ —ç—Ç—É –≤–µ—Ä—Å–∏—é)"
    )
    parser.add_argument(
        "--study-id",
        default="",
        help="UUID —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"
    )
    parser.add_argument(
        "--document-id",
        default="",
        help="UUID —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=600,
        help="–¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –∏–Ω–≥–µ—Å—Ç–∏–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 600)"
    )
    parser.add_argument(
        "--resume",
        action="store_true",
        help="–ü—Ä–æ–ø—É—Å–∫–∞—Ç—å —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (–ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ SHA256 –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö) –∏ –Ω–∞—á–∏–Ω–∞—Ç—å —Å –ø–µ—Ä–≤–æ–≥–æ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="–í—ã–≤–æ–¥–∏—Ç—å –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (—Ö–µ—à–∏ —Ñ–∞–π–ª–æ–≤, —Å—Ç–∞—Ç—É—Å—ã –∏ —Ç.–¥.)"
    )
    
    args = parser.parse_args()
    
    api_base = args.api.rstrip("/")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ workspace_id
    workspace_id = args.workspace_id.strip()
    version_id = args.version_id.strip()
    
    if not version_id:
        if not workspace_id:
            workspace_id = input("–í–≤–µ–¥–∏—Ç–µ WORKSPACE_ID (UUID): ").strip()
        if len(workspace_id) != 36:
            die(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç WORKSPACE_ID (–æ–∂–∏–¥–∞–µ—Ç—Å—è UUID –∏–∑ 36 —Å–∏–º–≤–æ–ª–æ–≤): {workspace_id}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å
    input_path = Path(args.path)
    if not input_path.is_absolute():
        # –ï—Å–ª–∏ –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π, –¥–µ–ª–∞–µ–º –µ–≥–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        input_path = Path.cwd() / input_path
    
    # –ù–∞—Ö–æ–¥–∏–º —Ñ–∞–π–ª—ã
    files = find_document_files(input_path)
    
    if not files:
        die(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤: {input_path}")
    
    # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º --resume, –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    processed_hashes: Set[str] = set()
    if args.resume:
        if not workspace_id:
            die("–î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è --resume –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å --workspace-id")
        print(f"\n{'='*80}")
        print("–ü–†–û–í–ï–†–ö–ê –û–ë–†–ê–ë–û–¢–ê–ù–ù–´–• –§–ê–ô–õ–û–í")
        print(f"{'='*80}")
        print(f"–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        processed_hashes = get_processed_sha256_set(api_base, workspace_id, debug=args.debug)
        print()
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ñ–∞–π–ª—ã, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º --resume
    files_to_process: List[Path] = []
    skipped_count = 0
    start_index = 0
    
    if args.resume and processed_hashes:
        print(f"\n{'='*80}")
        print("–ü–†–û–í–ï–†–ö–ê –§–ê–ô–õ–û–í –ù–ê –û–ë–†–ê–ë–û–¢–ê–ù–ù–û–°–¢–¨")
        print(f"{'='*80}")
        
        for idx, file_path in enumerate(files):
            try:
                file_sha256 = calculate_file_sha256(file_path)
                file_sha256_lower = file_sha256.lower()
                
                if file_sha256_lower in processed_hashes:
                    print(f"  [{idx + 1}/{len(files)}] –ü—Ä–æ–ø—É—â–µ–Ω (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω): {file_path.name}")
                    print(f"      SHA256: {file_sha256[:16]}...")
                    skipped_count += 1
                else:
                    if start_index == 0:
                        start_index = idx
                    files_to_process.append(file_path)
                    print(f"  [{idx + 1}/{len(files)}] –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {file_path.name}")
                    print(f"      SHA256: {file_sha256[:16]}... (–Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –±–∞–∑–µ)")
            except Exception as e:
                print(f"  [{idx + 1}/{len(files)}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ {file_path.name}: {e}")
                # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –¥–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–π–ª –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                if start_index == 0:
                    start_index = idx
                files_to_process.append(file_path)
        
        print(f"\n–ü—Ä–æ–ø—É—â–µ–Ω–æ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö: {skipped_count}")
        print(f"–ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(files_to_process)}")
        
        if not files_to_process:
            print(f"\n{'='*80}")
            print("–í–°–ï –§–ê–ô–õ–´ –£–ñ–ï –û–ë–†–ê–ë–û–¢–ê–ù–´")
            print(f"{'='*80}")
            sys.exit(0)
        
        if start_index > 0:
            print(f"\n–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å —Ñ–∞–π–ª–∞ {start_index + 1} –∏–∑ {len(files)}")
        
        files = files_to_process
    else:
        files_to_process = files
    
    print(f"\n{'='*80}")
    print(f"–ù–ê–ô–î–ï–ù–û –§–ê–ô–õ–û–í –î–õ–Ø –û–ë–†–ê–ë–û–¢–ö–ò: {len(files)}")
    if args.resume and skipped_count > 0:
        print(f"–ü–†–û–ü–£–©–ï–ù–û –£–ñ–ï –û–ë–†–ê–ë–û–¢–ê–ù–ù–´–•: {skipped_count}")
    print(f"{'='*80}")
    for i, f in enumerate(files, 1):
        print(f"  {i}. {f}")
    print()
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã
    results: List[Tuple[str, bool, Dict[str, Any]]] = []
    
    for idx, file_path in enumerate(files, 1):
        print(f"\n{'='*80}")
        print(f"–û–ë–†–ê–ë–û–¢–ö–ê –§–ê–ô–õ–ê {idx}/{len(files)}: {file_path.name}")
        print(f"{'='*80}")
        
        study_id = args.study_id.strip() if args.study_id else None
        document_id = args.document_id.strip() if args.document_id else None
        vid = version_id if version_id else None
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É (–∫—Ä–æ–º–µ —Å–ª—É—á–∞—è, –∫–æ–≥–¥–∞ —É–∫–∞–∑–∞–Ω version_id)
        create_new_study = not vid
        
        version_id_result, success, stats = process_file(
            api_base=api_base,
            workspace_id=workspace_id,
            file_path=file_path,
            study_id=study_id,
            document_id=document_id,
            version_id=vid,
            create_new_study=create_new_study,
            ingestion_timeout=args.timeout,
        )
        
        results.append((file_path.name, version_id_result, success, stats))
    
    # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print(f"\n{'='*80}")
    print("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print(f"{'='*80}")
    
    total = len(results)
    successful = sum(1 for _, _, s, _ in results if s)
    failed = total - successful
    
    print(f"\n–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total} —Ñ–∞–π–ª(–æ–≤)")
    print(f"–£—Å–ø–µ—à–Ω–æ: {successful}")
    print(f"–° –æ—à–∏–±–∫–∞–º–∏: {failed}")
    
    print(f"\n{'='*80}")
    print("–î–ï–¢–ê–õ–ò –ü–û –§–ê–ô–õ–ê–ú:")
    print(f"{'='*80}")
    
    for filename, vid, success, stats in results:
        status_icon = "‚úì" if success else "‚úó"
        status_text = "–£–°–ü–ï–®–ù–û" if success else "–û–®–ò–ë–ö–ê"
        print(f"\n{status_icon} {status_text}: {filename}")
        print(f"  Version ID: {vid}")
        if stats:
            print(f"  –°—Ç–∞—Ç—É—Å: {stats.get('final_status', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            print(f"  –Ø–∫–æ—Ä–µ–π —Å–æ–∑–¥–∞–Ω–æ: {stats.get('anchors_created', 0)}")
            print(f"  –ß–∞–Ω–∫–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {stats.get('chunks_created', 0)}")
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —à–∞–≥–∞–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
            detailed = stats.get('detailed', {})
            
            # SoA
            soa = detailed.get('soa', {})
            if soa.get('detected'):
                print(f"  üìã SoA (Schedule of Activities):")
                print(f"     - –í–∏–∑–∏—Ç–æ–≤: {soa.get('visits_count', 0)}")
                print(f"     - –ü—Ä–æ—Ü–µ–¥—É—Ä: {soa.get('procedures_count', 0)}")
                print(f"     - –Ø—á–µ–µ–∫ –º–∞—Ç—Ä–∏—Ü—ã: {soa.get('matrix_cells', 0)}")
                if soa.get('confidence'):
                    print(f"     - –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {soa['confidence']:.2f}")
            else:
                print(f"  üìã SoA: –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω")
            
            # Chunks
            chunks = detailed.get('chunks', {})
            if chunks.get('created', 0) > 0:
                print(f"  üìë Chunks (Narrative Index): {chunks['created']} —Å–æ–∑–¥–∞–Ω–æ")
                if chunks.get('anchors_per_chunk_avg'):
                    print(f"     - –°—Ä–µ–¥–Ω–µ–µ anchors/chunk: {chunks['anchors_per_chunk_avg']}")
            
            # Facts
            facts = detailed.get('facts', {})
            if facts.get('total_extracted', 0) > 0:
                print(f"  üìä –§–∞–∫—Ç—ã (Rules-first): {facts['total_extracted']} –∏–∑–≤–ª–µ—á–µ–Ω–æ")
                needs_review = facts.get('needs_review', [])
                if needs_review:
                    print(f"     - –¢—Ä–µ–±—É—é—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏: {len(needs_review)}")
                    for fact_key in needs_review[:3]:
                        print(f"       ‚Ä¢ {fact_key}")
                    if len(needs_review) > 3:
                        print(f"       ... –∏ –µ—â—ë {len(needs_review) - 3}")
            
            # Section Mapping
            mapping = detailed.get('section_mapping', {})
            mapped = mapping.get('sections_mapped', 0)
            needs_review_map = mapping.get('needs_review', 0)
            if mapped > 0 or needs_review_map > 0:
                print(f"  üó∫Ô∏è  –ú–∞–ø–ø–∏–Ω–≥ —Å–µ–∫—Ü–∏–π:")
                print(f"     - –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ: {mapped}")
                if needs_review_map > 0:
                    print(f"     - –¢—Ä–µ–±—É—é—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏: {needs_review_map}")
            
            # Warnings
            warnings = stats.get('warnings', [])
            if warnings:
                print(f"  ‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π: {len(warnings)}")
                for w in warnings[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                    print(f"     - {w}")
                if len(warnings) > 3:
                    print(f"     ... –∏ –µ—â—ë {len(warnings) - 3}")
    
    # –ó–∞–≤–µ—Ä—à–∞–µ–º —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º
    if failed > 0:
        print(f"\n{'='*80}")
        print(f"–í–ù–ò–ú–ê–ù–ò–ï: {failed} —Ñ–∞–π–ª(–æ–≤) –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å —Å –æ—à–∏–±–∫–∞–º–∏")
        print(f"{'='*80}")
        sys.exit(1)
    else:
        print(f"\n{'='*80}")
        print("–í–°–ï –§–ê–ô–õ–´ –û–ë–†–ê–ë–û–¢–ê–ù–´ –£–°–ü–ï–®–ù–û")
        print(f"{'='*80}")


if __name__ == "__main__":
    main()

